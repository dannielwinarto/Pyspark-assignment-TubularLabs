{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1 and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init('/home/danniel/spark-2.1.0-bin-hadoop2.7')\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('Tub_lab').getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q2 - Load the two files into spark and join them by user_id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data1 = spark.read.csv(\"solutions_test_file1.csv\", header = True, inferSchema= True)\n",
    "data2 = spark.read.csv(\"solutions_test_file2.csv\", header = True, inferSchema= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- user_id: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- age_bin: string (nullable = true)\n",
      " |-- country: string (nullable = true)\n",
      "\n",
      "root\n",
      " |-- user_id: string (nullable = true)\n",
      " |-- category: string (nullable = true)\n",
      " |-- language: string (nullable = true)\n",
      " |-- creator_country: string (nullable = true)\n",
      " |-- comment_text: string (nullable = true)\n",
      " |-- _c5: string (nullable = true)\n",
      " |-- _c6: string (nullable = true)\n",
      " |-- _c7: string (nullable = true)\n",
      " |-- _c8: string (nullable = true)\n",
      " |-- _c9: string (nullable = true)\n",
      " |-- _c10: string (nullable = true)\n",
      " |-- _c11: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data1.printSchema()\n",
    "data2.printSchema() # we need to drop c5 to c11\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- user_id: string (nullable = true)\n",
      " |-- category: string (nullable = true)\n",
      " |-- language: string (nullable = true)\n",
      " |-- creator_country: string (nullable = true)\n",
      " |-- comment_text: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data2 = data2.drop('_c5', '_c6', '_c7','_c8','_c9', '_c10', '_c11')\n",
    "data2.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+-------+-------+-------------+--------+---------------+--------------------+\n",
      "|             user_id|gender|age_bin|country|     category|language|creator_country|        comment_text|\n",
      "+--------------------+------+-------+-------+-------------+--------+---------------+--------------------+\n",
      "|1445876f62cd03a0d...|     m|  35-44|     US|        Music|      en|             US|how you hug a pup...|\n",
      "|eaadfa19afdb902d0...|     m|  25-34|     NO|Entertainment|      en|             GB|               First|\n",
      "|8681d7a8a7306c4cf...|     m|  25-34|     US|Entertainment|      en|             US|I bet its so hot ...|\n",
      "+--------------------+------+-------+-------+-------------+--------+---------------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Joining by user_id\n",
    "full_data = data1.join(data2, data1.user_id == data2.user_id, how = 'inner').drop(data2.user_id)\n",
    "full_data.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q3 Compute percentages of categories by gender and report the top 3 categories for each gender as well as the percentages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import format_number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|            category|count|\n",
      "+--------------------+-----+\n",
      "|           Education|    5|\n",
      "|              Gaming|   23|\n",
      "|       Entertainment|   37|\n",
      "|     Travel & Events|    2|\n",
      "|Science & Technology|    3|\n",
      "|              Sports|    3|\n",
      "|       Howto & Style|   34|\n",
      "|    Film & Animation|    9|\n",
      "|      People & Blogs|   38|\n",
      "|     News & Politics|    4|\n",
      "|      Pets & Animals|    1|\n",
      "|               Music|    9|\n",
      "|              Comedy|    6|\n",
      "+--------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "full_data.filter(full_data['gender'] == 'f').groupBy('category').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "count_by_cat_gender = full_data.groupBy(['gender','category']).agg({'*':'count'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "826"
      ]
     },
     "execution_count": 266,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "men_women_count = full_data.groupBy(\"gender\").count().collect()\n",
    "men_women_count[1]\n",
    "men_women_count[0][1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------+-----+-----------------------+\n",
      "|gender|      category|count|percentage_among_female|\n",
      "+------+--------------+-----+-----------------------+\n",
      "|     f|People & Blogs|   38|                  21.84|\n",
      "|     f| Entertainment|   37|                  21.26|\n",
      "|     f| Howto & Style|   34|                  19.54|\n",
      "+------+--------------+-----+-----------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "count_by_cat_gender\\\n",
    "    .filter(count_by_cat_gender['gender'] == 'f')\\\n",
    "    .select([\"gender\", \n",
    "             \"category\",\n",
    "             count_by_cat_gender[\"count(1)\"].alias(\"count\"),\n",
    "             (format_number((100*count_by_cat_gender[\"count(1)\"]/men_women_count[1][1]),2)).cast(\"float\")])\\\n",
    "    .withColumnRenamed(\"CAST(format_number(((count(1) * 100) / 174), 2) AS FLOAT)\", \"percentage_among_female\")\\\n",
    "    .orderBy(\"percentage_among_female\", ascending = False).show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------------+-----+---------------------+\n",
      "|gender|       category|count|percentage_among_male|\n",
      "+------+---------------+-----+---------------------+\n",
      "|     m|         Gaming|  202|                24.46|\n",
      "|     m|  Entertainment|  196|                23.73|\n",
      "|     m|News & Politics|   92|                11.14|\n",
      "+------+---------------+-----+---------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "count_by_cat_gender\\\n",
    "    .filter(count_by_cat_gender['gender'] == 'm')\\\n",
    "    .select([\"gender\", \n",
    "             \"category\",\n",
    "             count_by_cat_gender[\"count(1)\"].alias(\"count\"),\n",
    "             (format_number((100*count_by_cat_gender[\"count(1)\"]/men_women_count[0][1]),2)).cast(\"float\")])\\\n",
    "    .withColumnRenamed(\"CAST(format_number(((count(1) * 100) / 826), 2) AS FLOAT)\", \"percentage_among_male\")\\\n",
    "    .orderBy(\"percentage_among_male\", ascending = False).show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q4 Using pyspark.ml.feature.Tokenizer split the comments into lists of words. Combine the lists of words from all comments into lists of words by gender and category. Report the top 3 most common words for gender='f' and category='People & Blogs', as well as the number of occurrences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Tokenizer,CountVectorizer\n",
    "from pyspark.sql.functions import collect_list, udf, explode, col\n",
    "from pyspark.sql.types import ArrayType, StringType,IntegerType,FloatType\n",
    "from pyspark.ml.linalg import SparseVector\n",
    "from itertools import chain\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+-------+-------+--------+--------+---------------+--------------------+\n",
      "|             user_id|gender|age_bin|country|category|language|creator_country|        comment_text|\n",
      "+--------------------+------+-------+-------+--------+--------+---------------+--------------------+\n",
      "|1445876f62cd03a0d...|     m|  35-44|     US|   Music|      en|             US|how you hug a pup...|\n",
      "+--------------------+------+-------+-------+--------+--------+---------------+--------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "full_data.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------------+--------------------+\n",
      "|gender|     category|        comment_text|\n",
      "+------+-------------+--------------------+\n",
      "|     m|        Music|how you hug a pup...|\n",
      "|     m|Entertainment|               First|\n",
      "+------+-------------+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Q4_full_data = full_data.select([\"gender\",\"category\",\"comment_text\"])\n",
    "Q4_full_data.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------------+--------------------+--------------------+\n",
      "|gender|     category|        comment_text|          tok_output|\n",
      "+------+-------------+--------------------+--------------------+\n",
      "|     m|        Music|how you hug a pup...|[how, you, hug, a...|\n",
      "|     m|Entertainment|               First|             [first]|\n",
      "|     m|Entertainment|I bet its so hot ...|[i, bet, its, so,...|\n",
      "+------+-------------+--------------------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tok = Tokenizer(inputCol=\"comment_text\", outputCol= \"tok_output\")\n",
    "Q4_data_tok = tok.transform(Q4_full_data)\n",
    "Q4_data_tok.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------------+--------------------+\n",
      "|gender|     category|       tok_collected|\n",
      "+------+-------------+--------------------+\n",
      "|     f|       Comedy|[WrappedArray(my,...|\n",
      "|     f|    Education|[WrappedArray(goi...|\n",
      "|     f|Entertainment|[WrappedArray(ala...|\n",
      "+------+-------------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# grouped words by gender and category\n",
    "Q4_data_agg = Q4_data_tok.groupBy([\"gender\",\"category\"])\\\n",
    "    .agg(collect_list(\"tok_output\"))\\\n",
    "    .orderBy([\"gender\",\"category\"])\\\n",
    "    .withColumnRenamed(\"collect_list(tok_output)\", \"tok_collected\")\n",
    "Q4_data_agg.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------------+--------------------+\n",
      "|gender|     category|         flat_output|\n",
      "+------+-------------+--------------------+\n",
      "|     f|       Comedy|[my, walls, as, a...|\n",
      "|     f|    Education|[goil, sounds, li...|\n",
      "|     f|Entertainment|[alaric, pls, bab...|\n",
      "+------+-------------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flattened  = udf(f= lambda x: list(chain.from_iterable(x)), \n",
    "                 returnType= ArrayType(StringType()))\n",
    "Q4_data_flatted = Q4_data_agg.withColumn(\"flat_output\", flattened(\"tok_collected\")).drop(\"tok_collected\")\n",
    "Q4_data_flatted.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------------+--------------------+--------------------+\n",
      "|gender|     category|         flat_output|           cv_output|\n",
      "+------+-------------+--------------------+--------------------+\n",
      "|     f|       Comedy|[my, walls, as, a...|(4440,[0,2,3,5,7,...|\n",
      "|     f|    Education|[goil, sounds, li...|(4440,[0,1,5,10,1...|\n",
      "|     f|Entertainment|[alaric, pls, bab...|(4440,[0,1,2,3,4,...|\n",
      "+------+-------------+--------------------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# CountVectorizer actually not needed in this Question 4\n",
    "cv = CountVectorizer(inputCol=\"flat_output\", outputCol=\"cv_output\")\n",
    "cv_model = cv.fit(Q4_data_flatted)\n",
    "Q4_vectorized = cv_model.transform(Q4_data_flatted)\n",
    "Q4_vectorized.show(3,truncate = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------+--------------------+--------------------+\n",
      "|gender|      category|         flat_output|           cv_output|\n",
      "+------+--------------+--------------------+--------------------+\n",
      "|     f|People & Blogs|[your, makeup, is...|(4440,[0,1,2,3,4,...|\n",
      "+------+--------------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Q4_vectorized_f_PnB = Q4_vectorized.filter((Q4_vectorized[\"gender\"] == \"f\") & (Q4_vectorized[\"category\"] == \"People & Blogs\"))\n",
    "Q4_vectorized_f_PnB.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|  word|\n",
      "+------+\n",
      "|  your|\n",
      "|makeup|\n",
      "+------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "Q4_vocab_f_PnB = Q4_vectorized_f_PnB.select(\"flat_output\").rdd.flatMap(lambda x: x[0])\\\n",
    "    .toDF(schema = StringType()).toDF('word')\n",
    "Q4_vocab_f_PnB.show(2)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word_female_PeopleNBlogs</th>\n",
       "      <th>ocurances</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>i</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>the</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>a</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   word_female_PeopleNBlogs  ocurances\n",
       "63                        i         15\n",
       "34                      the         13\n",
       "68                        a         12"
      ]
     },
     "execution_count": 278,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q4_vocab_freq_f_PnB = Q4_vocab_f_PnB.rdd.countByValue()\n",
    "Q4_df_answer = pd.DataFrame([[k[0],v] for k,v in Q4_vocab_freq_f_PnB.items()],\n",
    "                columns = [\"word_female_PeopleNBlogs\", \"ocurances\"])\n",
    "Q4_df_answer.sort_values('ocurances',ascending=False)[0:3]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q5 Compute the frequency of occurrence for each word, normalized by the total number of words for all gender / categories (call this f_all), as well as by the total number of words in each gender / category (call this f_gc). Compute the frequency ratio R = f_gc / f_all. Report the top 3 words ordering by R for gender='f' and category='People & Blogs' after imposing a minimum number of 5 overall occurrences (i.e. in all gender / categories)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------------+--------------------+--------------------+\n",
      "|gender|     category|        comment_text|          tok_output|\n",
      "+------+-------------+--------------------+--------------------+\n",
      "|     m|        Music|how you hug a pup...|[how, you, hug, a...|\n",
      "|     m|Entertainment|               First|             [first]|\n",
      "|     m|Entertainment|I bet its so hot ...|[i, bet, its, so,...|\n",
      "+------+-------------+--------------------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Since requirement of minimum 5 occurances, we need to find the list of word that pass this requirement first\n",
    "# by using CountVectorizer and set minDF = 5 , we can find the list of words that passed this requirement.\n",
    "# we can retrieve the list of passed words by using cv_model.vocabulary, where cv_model is our CountVectorizer model\n",
    "\n",
    "from pyspark.sql.functions import col\n",
    "Q4_data_tok.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------------+--------------------+--------------------+--------------------+\n",
      "|gender|     category|        comment_text|          tok_output|           cv_output|\n",
      "+------+-------------+--------------------+--------------------+--------------------+\n",
      "|     m|        Music|how you hug a pup...|[how, you, hug, a...|(297,[2,7,26],[1....|\n",
      "|     m|Entertainment|               First|             [first]|   (297,[120],[1.0])|\n",
      "|     m|Entertainment|I bet its so hot ...|[i, bet, its, so,...|(297,[1,2,8,14,15...|\n",
      "+------+-------------+--------------------+--------------------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cv = CountVectorizer(inputCol=\"tok_output\", outputCol=\"cv_output\", minDF= 5)\n",
    "cv_model = cv.fit(Q4_data_tok)\n",
    "Q5_vectorized = cv_model.transform(Q4_data_tok)\n",
    "cv_model.vocabulary\n",
    "Q5_vectorized.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+\n",
      "|word|\n",
      "+----+\n",
      "| how|\n",
      "| you|\n",
      "|   a|\n",
      "+----+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Q5_filtered_words = Q5_vectorized\\\n",
    "    .agg(collect_list('tok_output'))\\\n",
    "    .withColumn(\"flat_output\", flattened(\"collect_list(tok_output)\"))\\\n",
    "    .drop('collect_list(tok_output)')\\\n",
    "    .select(\"flat_output\")\\\n",
    "    .rdd\\\n",
    "    .flatMap(lambda x: x[0])\\\n",
    "    .toDF(schema = StringType())\\\n",
    "    .toDF('word')\\\n",
    "    .filter(col(\"word\").isin(cv_model.vocabulary))\n",
    "\n",
    "Q5_filtered_words.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>ocurances</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>wrong</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>274</th>\n",
       "      <td>time.</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>&lt;3</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      word  ocurances\n",
       "83   wrong          5\n",
       "274  time.          5\n",
       "40      <3          5"
      ]
     },
     "execution_count": 282,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# here we are going to check whether we successfully filter out the words that are < 5 document frequency\n",
    "Q5_word_freq_check = Q5_filtered_words.rdd.countByValue()\n",
    "Q5_df_word_freq_check = pd.DataFrame([[k[0],v] for k,v in Q5_word_freq_check.items()],\n",
    "                columns = [\"word\", \"ocurances\"])\n",
    "Q5_df_word_freq_check.sort_values('ocurances',ascending=True)[0:3]\n",
    "# Yes, we are confirmed that the minimum occurance is 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---------+--------------------+\n",
      "|word|ocurances|               f_all|\n",
      "+----+---------+--------------------+\n",
      "| the|      484|  0.0715658731332249|\n",
      "|   i|      297|0.043915422149933464|\n",
      "|   a|      266|0.039331657548425256|\n",
      "|  is|      201|0.029720538222682242|\n",
      "|  to|      192| 0.02838976785450244|\n",
      "| and|      185| 0.02735472423480704|\n",
      "|  of|      160|0.023658139878752035|\n",
      "| you|      156|0.023066686381783233|\n",
      "|that|      150|0.022179506136330032|\n",
      "|this|      142| 0.02099659914239243|\n",
      "|  in|      111|0.016412834540884224|\n",
      "|  it|      103| 0.01522992754694662|\n",
      "| for|       88|0.013011976933313619|\n",
      "| was|       84|0.012420523436344817|\n",
      "|like|       79|0.011681206565133816|\n",
      "|  so|       77|0.011385479816649415|\n",
      "|  on|       69|0.010202572822711814|\n",
      "| not|       69|0.010202572822711814|\n",
      "|  be|       66|0.009758982699985213|\n",
      "|just|       64|0.009463255951500812|\n",
      "+----+---------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Q5_f_all = (spark.createDataFrame(Q5_df_word_freq_check))\\\n",
    "    .withColumn(\"f_all\", col('ocurances')/Q5_filtered_words.count())\\\n",
    "    .orderBy('f_all', ascending = False)\n",
    "Q5_f_all.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------+--------------------+--------------------+--------------------+\n",
      "|gender|      category|        comment_text|          tok_output|           cv_output|\n",
      "+------+--------------+--------------------+--------------------+--------------------+\n",
      "|     f|People & Blogs|YOUR MAKEUP IS PR...|[your, makeup, is...|(297,[3,39],[1.0,...|\n",
      "|     f|People & Blogs|           beauty���|         [beauty���]|         (297,[],[])|\n",
      "|     f|People & Blogs|Angel!!!!! She lo...|[angel!!!!!, she,...|(297,[0,15,60,85]...|\n",
      "+------+--------------+--------------------+--------------------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# next we're going to work on f_gc, which is \"female\" & \"people&Blogs\"\n",
    "# the way we do it is very similar like finding f_all, except we have to filter by \"female\" and \"people&blogs\" \n",
    "Q5_vectorized_f_PnB = Q5_vectorized.filter((Q5_vectorized[\"gender\"] == \"f\") & (Q5_vectorized[\"category\"] == \"People & Blogs\"))\n",
    "Q5_vectorized_f_PnB.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+\n",
      "|word|\n",
      "+----+\n",
      "|your|\n",
      "|  is|\n",
      "| she|\n",
      "+----+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Q5_filtered_words_f_PnB = Q5_vectorized_f_PnB\\\n",
    "    .agg(collect_list('tok_output'))\\\n",
    "    .withColumn(\"flat_output\", flattened(\"collect_list(tok_output)\"))\\\n",
    "    .drop('collect_list(tok_output)')\\\n",
    "    .select(\"flat_output\")\\\n",
    "    .rdd\\\n",
    "    .flatMap(lambda x: x[0])\\\n",
    "    .toDF(schema = StringType())\\\n",
    "    .toDF('word')\\\n",
    "    .filter(col(\"word\").isin(cv_model.vocabulary))\n",
    "\n",
    "Q5_filtered_words_f_PnB.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word_f_PnB</th>\n",
       "      <th>ocurances_f_PnB</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>i</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>the</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>a</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   word_f_PnB  ocurances_f_PnB\n",
       "59          i               15\n",
       "26        the               13\n",
       "86          a               12"
      ]
     },
     "execution_count": 286,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# same procedure as previously to check document frequency within the subset\n",
    "Q5_word_freq_check_f_PnB = Q5_filtered_words_f_PnB.rdd.countByValue()\n",
    "Q5_df_word_freq_check_PnB = pd.DataFrame([[k[0],v] for k,v in Q5_word_freq_check_f_PnB.items()],\n",
    "                columns = [\"word_f_PnB\", \"ocurances_f_PnB\"])\n",
    "Q5_df_word_freq_check_PnB.sort_values('ocurances_f_PnB',ascending=False)[0:3]\n",
    "# it may seemed wrong, but remember that threshold 5 is limit of the ENTIRE document, \n",
    "# when we subset by gender and category, it is okay to have occurance < 5 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------------+-------------------+\n",
      "|word_f_PnB|ocurances_f_PnB|               f_gc|\n",
      "+----------+---------------+-------------------+\n",
      "|         i|             15|0.05415162454873646|\n",
      "|       the|             13|0.04693140794223827|\n",
      "|         a|             12|0.04332129963898917|\n",
      "+----------+---------------+-------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Q5_f_gc_f_PnB = (spark.createDataFrame(Q5_df_word_freq_check_PnB))\\\n",
    "    .withColumn(\"f_gc\", col('ocurances_f_PnB')/Q5_filtered_words_f_PnB.count())\\\n",
    "    .orderBy('f_gc', ascending = False)\n",
    "Q5_f_gc_f_PnB.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The next step is finding the R(where R = f_gc/f_all), since Q5_f_all and Q5_f_gc_f_PnB has different size, \n",
    "# we need to filter the Q5_f_all so it only contains words that within Q5_f_gc_f_PnB\n",
    "list_word_subset = [i.word for i in Q5_filtered_words_f_PnB.select('word').collect()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------------+--------------------+\n",
      "|word_f_PnB|ocurances_f_PnB|                f_gc|\n",
      "+----------+---------------+--------------------+\n",
      "|          |              7| 0.02527075812274368|\n",
      "|         \"|              1|0.003610108303249...|\n",
      "|        \"i|              1|0.003610108303249...|\n",
      "+----------+---------------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Q5_f_gc_f_PnB.orderBy(\"word_f_PnB\").show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---------+--------------------+\n",
      "|word|ocurances|               f_all|\n",
      "+----+---------+--------------------+\n",
      "|    |       37|0.005470944846961407|\n",
      "|   \"|        9|0.001330770368179802|\n",
      "|  \"i|        5|7.393168712110011E-4|\n",
      "+----+---------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Q5_f_all_f_PnB = Q5_f_all.filter(col('word')\\\n",
    "    .isin(list_word_subset))\\\n",
    "    .orderBy('word')\n",
    "    \n",
    "Q5_f_all_f_PnB.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# the final step is we just need to join both together and calculate R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------------+--------------------+---------+--------------------+------------------+\n",
      "|word_f_PnB|ocurances_f_PnB|                f_gc|ocurances|               f_all|                 R|\n",
      "+----------+---------------+--------------------+---------+--------------------+------------------+\n",
      "|       too|              3|0.010830324909747292|        7|0.001035043619695...|10.463641052088706|\n",
      "|       two|              2|0.007220216606498195|        5|7.393168712110011E-4| 9.766064981949459|\n",
      "|      live|              2|0.007220216606498195|        6|8.871802454532013E-4| 8.138387484957882|\n",
      "+----------+---------------+--------------------+---------+--------------------+------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Q5_answer = Q5_f_gc_f_PnB.join(Q5_f_all_f_PnB, \n",
    "                                Q5_f_gc_f_PnB.word_f_PnB == Q5_f_all_f_PnB.word, \n",
    "                                how = 'inner')\\\n",
    "    .drop(Q5_f_all_f_PnB.word)\\\n",
    "    .withColumn(\"R\", (col(\"f_gc\")/col(\"f_all\")))\\\n",
    "    .orderBy(\"R\", ascending = False)\n",
    "Q5_answer.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  \"too\", \"two\", and \"live\" are words with the highest R within the subset of f_PnB(F and People&blogs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Thank you so much for the chance of doing this challenge, this is pretty challenging task and I have learned a lot in just short period of time. Thank you so much for Ted and the team. Please feel free to email me dannielwinarto@gmail.com for further questions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
